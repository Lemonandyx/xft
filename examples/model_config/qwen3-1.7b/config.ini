[qwen3]
model_name = /data/Qwen3-1.7B
head_num = 16
kv_head_num = 8
hidden_size = 2048
size_per_head = 128
inter_size = 6144
max_pos_seq_len = 40960
num_layer = 28
rms_norm_eps = 1e-06
layernorm_type = pre_layernorm
activation_type = silu
rope_theta = 1000000
has_post_decoder_layernorm = 1
vocab_size = 151936
start_id = 151643
end_id = 151645
pad_id = 151645
weight_data_type = bf16
attn_params_type = GQAttnParams
ffn_params_type = LlamaFFNParams
do_qk_norm = 1

