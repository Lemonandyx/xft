[deepseek_moe]
model_name = /data/DeepSeek-R1
head_num = 128
kv_head_num = 128
hidden_size = 7168
size_per_head = 128
inter_size = 18432
max_pos_seq_len = 163840
num_layer = 61
layernorm_eps = 1e-06
layernorm_type = pre_layernorm
activation_type = silu
rope_theta = 10000
rope_scaling_factor = 40
rope_scaling_type = yarn
rope_scaling_original_max_position_embeddings = 4096
rope_scaling_beta_fast = 32
rope_scaling_beta_slow = 1
rope_scaling_mscale = 1.0
rope_scaling_mscale_all_dim = 1.0
has_post_decoder_layernorm = 1
vocab_size = 129280
start_id = 0
end_id = 1
pad_id = 1
weight_data_type = fp8_e4m3
attn_params_type = MLAttnParams
ffn_params_type = DeepSeekFFNParams
q_lora_rank = 1536
kv_lora_rank = 512
qk_rope_head_dim = 64
v_head_dim = 128
qk_nope_head_dim = 128
moe_intermediate_size = 2048
first_k_dense_replace = 3
dense_experts = 1
sparse_experts = 256
routed_scaling_factor = 2.5
topk_method = noaux_tc
n_group = 8
topk_group = 4
num_experts_per_tok = 8
norm_topk_prob = True
scoring_func = sigmoid

